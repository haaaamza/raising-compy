# -*- coding: utf-8 -*-
"""COMP551.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LP-JhNn4zf7j6Ir_I5cJq9l1OzFwc-4R

# Part One

## Package Imports
"""

#Import all necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import io
import seaborn as sns
from google.colab import files
from sklearn.preprocessing import OneHotEncoder
from sklearn import preprocessing

"""## Ionosphere"""

ionosphere_upload = files.upload()

ionosphere_data = pd.read_csv(io.StringIO(ionosphere_upload['ionosphere.data'].decode('utf-8')), header=None)
ionosphere_data

n = ionosphere_data.shape[1] - 1 #number of features (minus one for labels)
m = ionosphere_data.shape[0] #number of examples

ionosphere_data.rename(columns = {34:'label'}, inplace=True)
ionosphere_data.head()
#The even columns represent 17 sequential real number readouts from the radar system
#The odd columns are 17 sequential imaginary number readouts (corresponding to previous real readout)

ionosphere_data.info()
#There are no null features

ionosphere_data['label'].value_counts()
#No missing labels

ionosphere_data.describe()
#The second column is all zeroes. This is useless so we will drop it before training
#generally, features are bounded between -1 and 1
#The mean for the imaginary features is typically  smaller than those for the real features

#Let's look at the first few examples as a time series
fig, axs = plt.subplots(10, 1, sharex='col')

for i in range(10):
  example = list(ionosphere_data.loc[i])
  real = example[0:-1:2]
  imaginary = example[1::2]

  axs[i].plot(np.arange(len(real)), real, 'b', label="Real")
  axs[i].plot(np.arange(len(imaginary)), imaginary, 'r', label="Imaginary")
  axs[i].set_ylabel("Power")
  axs[i].legend()

  if example[-1] == 'g':
    plt.text(0.5, 0.8,'GOOD',
     transform = axs[i].transAxes)
  else:
    plt.text(0.5, 0.8,'BAD',
     transform = axs[i].transAxes)
  
fig.set_size_inches(10, 15)

#Bad examples seem to have higher variance than good examples
#Also, the distance between the real and imaginary series seems larger for good examples

good_ionosphere = ionosphere_data[ionosphere_data['label'] == 'g']
bad_ionosphere = ionosphere_data[ionosphere_data['label'] == 'b']
#Isolate the good and bad examples

#Here, we calculate some correlations
#First, we find the pearson's correlation coefficients between each pair of real and imaginary features
#Then, we plot the correlation between all different features for good examples and bad examples


## PEARSON'S CORRELATION FOR REAL/IMAGINARY PAIRS ##
correlations = []
for i in range(0, 33, 2):
  correlations.append(np.corrcoef(ionosphere_data[i], ionosphere_data[i+1])[1,0])

fig = plt.figure()
plt.plot(np.arange(17), correlations)
plt.title("Pearsons' Correlation Coefficients: Real vs. Imaginary")
plt.xlabel("Time Delay")
plt.ylabel("Pearson's Correlation")



## CORRELATION FOR GOOD AND BAD EXAMPLES ##

total_good_corr = good_ionosphere.drop([0, 1], axis=1, inplace=False).corr() #because the first two columns are always 1 or 0, we don't analyze them
total_bad_corr = bad_ionosphere.drop([0, 1], axis=1, inplace=False).corr()

fig2, ax2 = plt.subplots(2)
ax2[0].set_title("Good Examples: Correlation Heatmap")
ax2[1].set_title("Bad Examples: Correlation Heatmap")


sns.heatmap(total_good_corr, ax=ax2[0])
sns.heatmap(total_bad_corr, ax=ax2[1])


fig2.set_size_inches(10, 10)


#Notice the checkerboard pattern resulting from the structure of the data
#Each real time point correlates well with surrounding real time points, but not with adjacent imaginary ones
#Bad examples don't show structured correlatoin patterns

# With previous findings, autocorrelations of imaginary/real time series could be a valuable new features

normalized_autocorr_real = []
normalized_autocorr_im = []

for example in ionosphere_data.drop(labels=[0, 1, 'label'], axis=1).iterrows():
  real_series = example[1][::2]
  im_series = example[1][1::2]

  lag = 1
  normalized_autocorr_real.append(np.corrcoef(np.array([real_series[:-lag], real_series[lag:]]))[0,1])
  normalized_autocorr_im.append(np.corrcoef(np.array([im_series[:-lag], im_series[lag:]]))[0,1])

ionosphere_data['label'][np.isnan(np.array(normalized_autocorr_real))]
#Some null values appeared in calculating autocorrelatoin
#Check associated labels

ionosphere_data['label'][np.isnan(np.array(normalized_autocorr_im))]
#Check associated labels again

#Labels weren't consistently good or bad
#We will impute nan values with the mean of autocorrelations

autocorr_df = pd.DataFrame(data={'acorr_real': normalized_autocorr_real, 'acorr_im': normalized_autocorr_im}, dtype=float)
autocorr_df.fillna(autocorr_df.mean(), inplace=True)
autocorr_df.info()

autocorr_melted = pd.melt(pd.concat([autocorr_df, ionosphere_data['label']], axis=1), id_vars=["label"], value_vars=["acorr_real", "acorr_im"], var_name = "Classification", value_name = "Autocorrelation")
sns.boxplot(x="Classification", y="Autocorrelation", hue="label", data=autocorr_melted, palette="Set2")
#Good examples demonstrate higher autocorrelation than bad examples

#Show average time series for GOOD vs BAD examples
good_means = np.mean(good_ionosphere, axis=0)
bad_means = np.mean(bad_ionosphere, axis=0)

good_std = np.std(good_ionosphere, axis=0)
bad_std = np.std(bad_ionosphere, axis=0)

fig, axs = plt.subplots(2, sharex='col')
axs[0].errorbar(np.arange(17), good_means[0:-1:2], yerr = good_std[0:-1:2], label="Real")
axs[0].errorbar(np.arange(17), good_means[1::2], yerr = good_std[1::2], label="Imaginary")
axs[0].legend()
axs[0].set_title("Good Examples: Average Time Series")
axs[0].set_ylabel("Power $\pm$ 1SD")

axs[1].errorbar(np.arange(17), bad_means[0:-1:2], yerr = bad_std[0:-1:2], label="Real")
axs[1].errorbar(np.arange(17), bad_means[1::2], yerr = bad_std[1::2], label="Imaginary")
axs[1].legend()
axs[1].set_title("Bad Examples: Average Time Series")
axs[1].set_ylabel("Power $\pm$ 1SD")

fig.set_size_inches(10.0, 10.0)
fig.text(0.5, 0.07, 'Time', ha='center', va='center')

#This once again shows that good examples have real/imaginary series that are further apart

# As we saw earlier, standard deviation might be a discriminative features

real_std = np.std(ionosphere_data[ionosphere_data.columns[0:-3:2]], axis=1)
im_std = np.std(ionosphere_data[ionosphere_data.columns[1:-3:2]], axis=1)
std_df = pd.DataFrame({'std_real': real_std, 'std_im': im_std}, dtype=float)

std_melted = pd.melt(pd.concat([std_df, ionosphere_data['label']], axis=1), id_vars=["label"], value_vars=["std_real", "std_im"], var_name = "Classification", value_name = "Standard Deviation")
sns.boxplot(x="Classification", y="Standard Deviation", hue="label", data=std_melted, palette="Set2")

plt.title("Standard Deviations: Good vs. Bad Examples")

#Good examples have LOWER standard deviation than bad examples

# Show distributions of average real/imaginary time series by label

real_mean = np.mean(ionosphere_data[ionosphere_data.columns[0:-1:2]], axis=1)
im_mean = np.mean(ionosphere_data[ionosphere_data.columns[1::2]], axis=1)

dist_df = pd.concat([real_mean, im_mean, ionosphere_data['label']], axis=1)
dist_df.columns = ["Real Mean", "Imaginary Mean", "Label"]
dist_df["Label"][dist_df["Label"]=='g'] = "Good"
dist_df["Label"][dist_df["Label"]=='b'] = "Bad"
sns.pairplot(dist_df, hue='Label', height=3, aspect=1.5)

#We can see that good examples, have very discriminative real means
#Imaginary means have similar distributions, but with higher variance in bad examples

fig = sns.countplot(x='label', data=ionosphere_data)
plt.xticks(np.arange(2), ["Good", "Bad"])
plt.title("Label Distribution")

#More bad examples than good examples

for col in range(2, ionosphere_data.shape[1] - 1, 2):
  sns.distplot(ionosphere_data[ionosphere_data.columns[col]], hist=False)

plt.xlabel("Power")
plt.ylabel("Frequency")
plt.title("Distributions of Real Variables")


#For all real variables, these are their distributions

for col in range(3, ionosphere_data.shape[1] - 1, 2):
  sns.distplot(ionosphere_data[ionosphere_data.columns[col]], hist=False)

plt.xlabel("Power")
plt.ylabel("Frequency")
plt.title("Distributions of Imaginary Variables")

#Same for imaginary variables

ionosphere_data.shape

ionosphere_labels_df = ionosphere_data['label']
ionosphere_df = ionosphere_data.drop([1, 'label'], axis=1)
ionosphere_new_features_df = pd.concat([autocorr_df, std_df], axis=1)
ionosphere_df

#As a last layer of preprocessing, apply standard scaling to each feature
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

ionosphere = scaler.fit_transform(ionosphere_df).astype(float)
ionosphere_new_features = scaler.fit_transform(ionosphere_new_features_df)
ionosphere_labels = ionosphere_labels_df.to_numpy().reshape(-1, 1)


#Encode labels
ionosphere_labels[ionosphere_labels == 'g'] = 1
ionosphere_labels[ionosphere_labels == 'b'] = 0
ionosphere_labels = ionosphere_labels.astype(int)

for col in range(2, ionosphere.shape[1], 2):
  sns.distplot(ionosphere[:, col], hist=False)

plt.xlabel("Power")
plt.ylabel("Frequency")
plt.title("Distributions of Standardized Real Variables")

#After normalization, the means clearly shift to zero

for col in range(3, ionosphere.shape[1], 2):
  sns.distplot(ionosphere[:, col], hist=False)

plt.xlabel("Power")
plt.ylabel("Frequency")
plt.title("Distributions of Standardized Imaginary Variables")

ionosphere.shape

"""## Adult Data Set"""

adult_upload = files.upload()
adult_col_names = ["age", "workclass", "final_weight", "education", "education_num", 
                   "marital_status", "occupation", "relationship", "race", 
                   "sex", "capital_gain", "capital_loss", "hours_per_week",
                   "native_country", "income"]
adult_data = pd.read_csv(io.StringIO(adult_upload['adult.data'].decode('utf-8')), header=None, names=adult_col_names)

adult_test_upload = files.upload()
adult_test = pd.read_csv(io.StringIO(adult_test_upload['adult.test'].decode('utf-8')), header=None, names=adult_col_names, skiprows=1)

#We merge the train and test sets and preprocess everything together
adult_data = pd.concat([adult_data, adult_test])
adult_data.reset_index(drop=True, inplace=True)
adult_backup = adult_data.copy()
adult_data

adult_data.describe()

adult_data.info()
#No null entries

adult_data['income'].value_counts()
#Fix discrepancies!

adult_data['income'].replace({' <=50K.': ' <=50K', ' >50K.': ' >50K'}, inplace=True)
adult_data['income'].value_counts()

fig = sns.countplot(x="income", data=adult_data)
plt.title("Adult Census Label Distribution")
#Many more sub 50K than above 50K

adult_data['education'].value_counts()

adult_data['education_num'].value_counts()
#These correspond perfectly to the categorical education variable

# 1:preschool
# 2:1st-4th
# 3:5th-6th
# 4:7th-8th
# 5:9th
# 6:10th
# 7:11th
# 8:12th
# 9:HS-grad
# 10:Some-College
# 11:Assoc-voc
# 12:Assoc-acdm
# 13:Bachelors
# 14:Masters
# 15:Prof-schoo
# 16:Doctorate

#Although the ascending numerical representation could represent higher educational achievement,
#It may be inaccurate to represent educational levels as evenly spaced numerical values
#It's safer to do one-hot encoding!

#We will drop the education_num column before training

sorted_education = [x for _, x in sorted(zip(adult_data["education_num"].value_counts().index,
                                       adult_data["education"].value_counts().index))]

plt.figure(figsize=(8, 6))
fig = sns.countplot(x="education_num", hue="income", data=adult_data)
plt.xticks(rotation=90)
plt.xlabel("education")
plt.title("Label Distribution in Education")

_ = fig.set_xticklabels(sorted_education)


#This shows the education distribution between the two labels

#To clarify the visual, we also give a normalized version
#This is done for a collection of different features so we modularize

def normalized_count_plot(x, y, data):
  fig = data.groupby(x)[y].value_counts(normalize=True).mul(100)\
    .rename('percent').reset_index()\
    .pipe((sns.catplot,'data'), x=x,y='percent',hue=y,kind='bar')

  _ = [plt.setp(ax.get_xticklabels(), rotation=90) for ax in fig.axes.flat]
  return fig

fig = normalized_count_plot('education_num', 'income', adult_data)
plt.title("Normalized Income Distribution by Education")
_ = fig.set_xticklabels(sorted_education)

adult_data["native_country"].value_counts()
#Lots of different features! Concern of dimensionality issues upon training
#857 malformed examples

adult_data = adult_data[adult_data['native_country'] != ' ?'] 
#drop those examples
adult_data.reset_index(drop=True, inplace=True)

#For visualizatoin, map countries to their geographical region
#Upon training, this could be a way to get around dimensionality issues as well
#Will be explored in feature selection

country_mapping = {
    ' United-States': "northern_america",
    ' Canada': "northern_america",
    ' Mexico': "central_america",
    " Puerto-Rico": "carribean",
    " India": "southern_asia",
    " El-Salvador": "central_america",
    " Cuba": "carribean",
    " England": "western_europe",
    " Jamaica": "carribean",
    " Italy": "southern_europe",
    " China": "eastern_europe",
    " Dominican-Republic": "carribean",
    " Vietnam": "southeast_asia",
    " Guatemala": "central_america",
    " Japan": "eastern_asia",
    " Columbia": "south_america",
    " Poland": "eastern_europe",
    " Haiti": "carribean", 
    " Iran": "western_asia",
    " Taiwan": "eastern_asia",
    " Portugal": "southern_europe",
    " Nicaragua": "central_america",
    " Peru": "south_america",
    " Greece": "southern_europe",
    " Ecuador": "south_america",
    " France": "western_europe",
    " Ireland": "western_europe",
    " Hong": "eastern_asia",
    " Trinadad&Tobago": "carribean",
    " Cambodia": "southeast_asia",
    " Laos": "southeast_asia",
    " Thailand": "southeast_asia",
    " Yugoslavia": "eastern_europe",
    " Hungary": "eastern_europe",
    " Honduras": "central_america",
    " Scotland": "western_europe",
    " Holand-Netherlands": "western_europe",
    " Philippines": "southeast_asia",
    " Germany": "western_europe",
    " Outlying-US(Guam-USVI-etc)": "misc",
    " South": "misc"
}

native_region = adult_data["native_country"].replace(country_mapping)
native_region.rename("native_region", inplace=True)
native_region.value_counts()

native_region_df = pd.concat([native_region, adult_data["income"]], axis=1)
fig = sns.countplot(x="native_region", hue="income", data=native_region_df)
_ = plt.xticks(rotation=90)
#not super helpful so plot normalized version

normalized_count_plot('native_region', 'income', native_region_df)
plt.title("Normalized Income Distribution by Native Region")
#notice the lower income in the carribean, south america, central america

adult_data["age"].describe()

sns.set_style('whitegrid')
sns.countplot(x='age',hue='income', data=adult_data,palette='RdBu_r')
#Younger Population makes less money, but as population grows that gap between <=50K and >50K lessens, meaning people get paid more as they age.

target_1 = adult_data.loc[adult_data['income'] == " <=50K"]
target_0 = adult_data.loc[adult_data['income'] == " >50K"]

sns.distplot(target_1[['age']], hist=False, rug=True, label="<=50K")
sns.distplot(target_0[['age']], hist=False, rug=True, label=">50K")

plt.title("Age Distributions by Label")
plt.xlabel("Age")
plt.ylabel("Power")

adult_data["workclass"].value_counts()
#2753 more examples with missing features

adult_data = adult_data[adult_data['workclass'] != ' ?'] #drop those rows
adult_data.reset_index(drop=True, inplace=True) 
adult_data["workclass"].value_counts()

normalized_count_plot('workclass', 'income', adult_data)
plt.title("Normalized Income Distribution by Workclass")

fig = adult_data.groupby(["marital_status", 'sex'])["income"].value_counts(normalize=True).mul(100)\
.rename('percent').reset_index().iloc[1::2]\
.pipe((sns.catplot,'data'), x='marital_status',y='percent',hue='sex',kind='bar')

_ = [plt.setp(ax.get_xticklabels(), rotation=90) for ax in fig.axes.flat]
plt.title("% >50K by Marital Status and Sex")

#In marriages, the women seem to be making more money
#In every other situation, men are earning more

normalized_count_plot("relationship", "income", adult_data)
plt.title("Normalized Income Distribution by Relationship")
#Marriages make money!

normalized_count_plot("race", "income", adult_data)
plt.title("Normalized Income Distribution by Race")

adult_data["occupation"].value_counts()
#Ten more malformed examples

adult_data = adult_data[adult_data["occupation"] != " ?"]
adult_data.reset_index(drop=True, inplace=True)

normalized_count_plot("occupation", "income", adult_data)
plt.title("Normalized Income Distribution by Occupation")

target_0 = adult_data.loc[adult_data['income'] == " >50K"]
target_1 = adult_data.loc[adult_data['income'] == " <=50K"]

sns.distplot(target_1[['hours_per_week']], hist=False, rug=True, label="<=50K")
sns.distplot(target_0[['hours_per_week']], hist=False, rug=True, label=">50K")

plt.title("Weekly Hours Distributions by Label")
plt.xlabel("Hours per Week")
plt.ylabel("Power")

adult_data["final_weight"].describe()

fnlwgt = adult_data.loc[:, ["final_weight", "income"]]
sns.pairplot(fnlwgt, hue="income")
plt.title("Distributions of Final Weight by Income")
#Final weight is a property of the census, not the actual people
#Evidently it's not discriminative!

adult_data['capital_gain'].value_counts()

adult_data['capital_loss'].value_counts()

#To explore some of the numerical correlations, look at age, hours per week, and net capital
#Combining capital gain and loss allows us to be more concise
adult_num = adult_data.loc[:, ["age", "hours_per_week", "education_num"]]
adult_num["net_capital"] = adult_data["capital_gain"] - adult_data["capital_loss"]
adult_num["income"] = adult_data["income"]
g = sns.pairplot(adult_num, hue='income')
g.fig.suptitle("Numerical Features: Scatter Plots and Distributions", y=1.04)

#No combination makes the labels noticeably separable
#Those with high net capital all make more money

adult_num["income"].replace({" >50K": 1, " <=50K": 0}, inplace=True)

numerical_correlation = adult_num.corr()
sns.heatmap(numerical_correlation, annot=True)
plt.title("Correlation Heatmap: Numerical Data")
#We can see some mild correlation between greater hours, higher net capital, age and income

sns.set_style('whitegrid')
sns.barplot(x='occupation', y='hours_per_week', hue='income',data=adult_data ,palette='RdBu_r')
#Richer people work more hours_per_week, and in the survey people in "farming-fishing" have highest frequency >50K
_ = plt.xticks(rotation=90)

sns.barplot(x='marital_status', y='age', hue='income',data=adult_data ,palette='BrBG')
_ = plt.xticks(rotation=90)

adult_data["education_num"].hist(bins=16)
plt.xlabel("Level of Education Achieved")
plt.ylabel("Frequency")
plt.title("Educational Achievement Distribution")

adult_data["income"].replace({" >50K": 1, " <=50K": 0}, inplace=True)

adult_df = adult_data.copy()
adult_df

adult_labels_df = adult_df.loc[:, "income"]
adult_df.drop(["income", "education_num"], axis=1, inplace=True)
# we one-hot encode education instead of using education-num
adult_df.info()

adult_regions_df = adult_df.drop("native_country", axis=1)
adult_regions_df["native_region"] = native_region_df['native_region']
adult_regions_df

adult_df = pd.get_dummies(adult_df)
adult_df.shape

adult_df

adult_regions_df = pd.get_dummies(adult_regions_df)
adult_regions_df.shape
#This encoding reduces the number of features a lot; we'll compare the two on performance in part 3

adult_regions_df

adult = adult_df.to_numpy()
adult_regions = adult_regions_df.to_numpy()
adult_labels = adult_labels_df.to_numpy().reshape(-1, 1)

adult_numeric_ix = [0, 1, 2, 3, 4] #normalize these columns
from sklearn.compose import ColumnTransformer

adult_processor = ColumnTransformer([
                                     ('scaler', StandardScaler(), adult_numeric_ix)
], remainder='passthrough')

adult = adult_processor.fit_transform(adult)
adult_regions = adult_processor.fit_transform(adult_regions)

"""## Breast Cancer Dataset"""

breastcancer_upload = files.upload()

breastcancer_data = pd.read_csv(io.StringIO(breastcancer_upload['breast-cancer.data'].decode('utf-8')), header=None)
breastcancer_data

breastcancer_data.columns=['Class', 'age', 'menopause', 'tumor-size', 'inv-nodes','node-caps','deg-malig', 'breast', 'breast-quad', 'irradiat']
breastcancer_data.info()
#No Null entries

breastcancer_data['node-caps'].value_counts()
#8 malformed examples

breastcancer_data = breastcancer_data[breastcancer_data['node-caps'] != '?'] #drop those rows
breastcancer_data.reset_index(drop=True, inplace=True)

breastcancer_data.describe()
#only one numeric feature

sns.countplot(x="Class", data=breastcancer_data)
plt.title("Breast Cancer Label Distribution")
#Mostly negative examples

breastcancer_data["age"].value_counts()

sns.countplot(x='age', hue="Class", data=breastcancer_data, 
              order = sorted(breastcancer_data["age"].value_counts().index))
plt.title("Age Distribution")
#There's only one individual < 30
#Having such non-representative features might bias something like Naive bayes
#A future direction might be to bin the age distributions to sub 30, 40-49, 50-59, and >60

sns.violinplot(x='age', y= 'deg-malig', hue="Class", 
               order = sorted(breastcancer_data["age"].value_counts().index),
                              data=breastcancer_data, split=True)
plt.title("Age and Malignancy Level")
#Shows that higher malignancy levels indicate higher chance of recurrence
#Notice that the amount of tier 3 tumors increases with age

tumor_size_nat_order = sorted(breastcancer_data["tumor-size"].value_counts().index, reverse=True)
tumor_size_nat_order.insert(-1, tumor_size_nat_order.pop(1)) #5-9 is automatically put near front
tumor_size_nat_order
sns.catplot(x="deg-malig", y="tumor-size", order = tumor_size_nat_order,
            hue='Class', kind="swarm", dodge=True, data=breastcancer_data)
plt.title("Tumor Size and Malignancy")

breastcancer_data['inv-nodes'].value_counts()

invasive_node_ordering = sorted(breastcancer_data['inv-nodes'].value_counts().index)
for i in range(3):
  invasive_node_ordering.insert(1, invasive_node_ordering.pop(-1))

invasive_node_ordering

nodes_grouping = breastcancer_data.groupby(["inv-nodes", 'node-caps'])["Class"].value_counts(normalize=True).mul(100)\
.rename('percent').reset_index()

nodes_grouping = nodes_grouping[nodes_grouping["Class"] == "recurrence-events"].reset_index()
fig = nodes_grouping.pipe((sns.catplot, 'data'), x='inv-nodes',y='percent',hue='node-caps',kind='bar',
      order = invasive_node_ordering)

_ = [plt.setp(ax.get_xticklabels(), rotation=90) for ax in fig.axes.flat]
plt.title("% Recurrence with Affected Lymph Nodes and Containment")

#Eventually, as more lymph nodes are invaded the breast cancer becomes contained
#This may be due to saturation of the nodes?

breastcancer_data['breast-quad'].value_counts()
#one more malformed example

breastcancer_data = breastcancer_data[breastcancer_data['breast-quad'] != '?']
breastcancer_data.reset_index(drop=True, inplace=True)
breastcancer_data['breast-quad'].value_counts()

sns.countplot(x='breast-quad', hue='Class', data=breastcancer_data)
plt.title("Class Distribution by Breast Quadrant")

ax=sns.countplot(x="irradiat", hue="Class", data=breastcancer_data);
plt.title('Count of Irradition treatement and Recurrence of Tumor')
#People that get irradiation treatment are more likely to have had recurrence events

target_1 = breastcancer_data.loc[breastcancer_data['Class'] == "no-recurrence-events"]
target_0 = breastcancer_data.loc[breastcancer_data['Class'] == "recurrence-events"]

sns.distplot(target_1[['deg-malig']], hist=False, rug=True, label="no-recurrence-events")
sns.distplot(target_0[['deg-malig']], hist=False, rug=True, label="recurrence-events")

plt.title("Malignancy Level Distributions by Label")
plt.xlabel("Malignancy Level")
plt.ylabel("Frequency")

ax=sns.countplot(x="node-caps", hue="Class", data=breastcancer_data);
plt.title('Node-Caps and Recurrence')

ax=sns.countplot(x="menopause", hue="Class", data=breastcancer_data);
plt.title('Menopause and Recurrence')

breastcancer_df = breastcancer_data.copy()
breastcancer_labels_df = breastcancer_df['Class']
breastcancer_labels_df.replace({'recurrence-events': 1, 'no-recurrence-events': 0}, inplace=True)
breastcancer_df.drop("Class", axis=1, inplace=True)
breastcancer_df = pd.get_dummies(breastcancer_df, columns = breastcancer_df.columns)
#Since every feature in this example represents a classification, we encode everything
#This incldues the deg-malig feature which is numeric, but really represents three classes of severity
breastcancer_df.shape
#41 features

breastcancer = breastcancer_df.to_numpy()
breastcancer_labels = breastcancer_labels_df.to_numpy().reshape(-1, 1)
#Numpy arrays ready for training

"""## Banknote Authentification"""

banknote_upload = files.upload()

banknote_data = pd.read_csv(io.StringIO(banknote_upload['data_banknote_authentication.txt'].decode('utf-8')), header=None)
banknote_data

banknote_data.columns=['variance', 'skewness', 'kurtosis', 'entropy', 'label']

banknote_data.info()
#No null entries

banknote_data.describe()

sns.countplot(x='label', data=banknote_data)
plt.title("Label Distribution")

g = sns.pairplot(data=banknote_data, hue='label')
g.fig.suptitle("Feature Distribution and Correlation", y=1.04)
#No observable outliers

banknote_correlation = banknote_data.corr()
sns.heatmap(banknote_correlation, annot=True)
plt.title("Banknote Feature and Label Correlation Heatmap")

banknote_labels_df = banknote_data['label']
banknote_labels = banknote_labels_df.to_numpy().reshape(-1, 1)
banknote_df = banknote_data.drop('label', axis=1)
banknote = banknote_df.to_numpy()

banknote_scaler = StandardScaler()
banknote = banknote_scaler.fit_transform(banknote) #normalize all features

"""# Part 2

## Logistic Regression
"""

class LogisticRegression:
  

  def __init__(self, learning_rate=0.01, grd_iter=1000, verbose=False, 
               lambda_penalty=0, elastic_ratio=0, tol=1e-5):
    '''
    0 <= elastic_ratio <= 1
    If elastic_ratio is 0, that means we have ALL L2 regularization
    If elastic_ratio is 1, that means we have ALL L1 regularization

    elastic_ratio determines the split between L1 and L2 regularization,
    essentially splitting the lambda_penalty parameter between them by some ratio

    tol will automatically stop training when the training is decrementing by a smaller value
    training is hardcapped by grd_iter for iterations
    '''
    assert 0 <= elastic_ratio and elastic_ratio <= 1, "Must provide elastic_ratio between 0 and 1 (inclusive)"
    assert learning_rate > 0 and grd_iter > 0 and tol > 0, "Must pass a positive learning_rate, grd_iter, tol"
    assert isinstance(grd_iter, int), "grd_iter must be an integer"
    assert lambda_penalty >= 0, "lambda_penatly must be greater than or equal to zero"


    #Initialize all parameters
    self.tol = tol
    self.learning_rate = learning_rate
    self.grd_iter = grd_iter
    self.fitted= False #This boolean is used to prevent predicting before fitting the model
    self.w = None #The size of this is determined when data is passed to fit
    self.verbose = verbose #Whether or not the model prints updates throughout training
    self.l1_lambda = (elastic_ratio) * lambda_penalty #Regularization split
    self.l2_lambda = (1 - elastic_ratio) * lambda_penalty
    
  def fit(self, X, y, add_bias=True):
    if add_bias:
      X = np.c_[np.ones((X.shape[0], 1)), X] 
      # The class is designed to work with datasets as-is (i.e. you don't have to manually add bias feature)
      # The model will automatically add the bias during fit or predict calls

    cost_array = [] # Fit returns the training cost at each iteration automatically
    self.fitted = True
    m = X.shape[0] #Num examples
    n = X.shape[1] #Num features (including bias)
    num_classes = 1 #assuming binary classification problem
    self.w = np.zeros((n, num_classes)) #Initialize weights to zero
    diff = float('inf') #Diff is what we compare to tol at each iteration
    J_prev = 0
    for i in range(self.grd_iter):
      if diff < self.tol: #Stop early
        if self.verbose:
          print(f"Stopping due to tolerance at iteration {i}...")
        break

      J = self.cost(X, y)
      diff = abs(J - J_prev)
      J_prev = J
      cost_array.append(J)

      if (self.verbose and i % 50 == 0):
        print(f"Iteration: {i+1} | Cost: {J}")

      grad = self.gradients(X, y)

      self.w = self.w - (self.learning_rate * (grad)) #updating...
    
    return cost_array
    
  def gradients(self, X, y):
    m = X.shape[0]
    y_pred = self.sigmoid(X)
    grad = (1/m) * np.dot(X.T, (np.subtract(y_pred, y)))
    grad += np.vstack((np.array([0]).reshape(1, 1), self.l1_lambda * np.sign(self.w[1:, 0].reshape(-1, 1))))
    grad +=  np.vstack((np.array([0]).reshape(1, 1), self.l2_lambda * self.w[1:, 0].reshape(-1, 1)))
    return grad

  def predict(self, X, add_bias=True):
    if (not self.fitted):
      print("Must FIT Logistic Regression before calling predict")
      return
    else:
      if add_bias:
        X = np.c_[np.ones((X.shape[0], 1)), X] #ADD BIAS
      y_pred = self.sigmoid(X)
      y_pred = y_pred >= 0.5
      return y_pred

  def sigmoid(self, X):
    z = np.dot(X, self.w)
    return 1/(1+np.exp(-z))

  def cost(self, X, y):
    z = np.dot(X,self.w) #N x 1
    J = np.mean( y * np.log1p(np.exp(-z)) + (1-y) * np.log1p(np.exp(z)) )
    J +=  self.l2_lambda * (np.linalg.norm(self.w[1:, 0], 2, axis=0)**2)
    J +=  self.l1_lambda * np.linalg.norm(self.w[1:, 0], 1, axis=0)
    return J

np.vstack((np.array([0]), lr.w[1:, 0]))
# lr.w[1:, 0].shape

"""## Naive Bayes"""

class NaiveBayes():

  def __init__(self, verbose=False, eps=1e-6):#, multinomial_ix=[]):
    self.fitted = False
    self.verbose = verbose

    self.bernoulli_ix = []
    self.gaussian_ix = []

    self.bernoulli_likelihood_w = None
    self.gaussian_likelihood_mu = None
    self.gaussian_likelihood_s = None
    self.class_prior = None

    self.bernoulli_features = None
    self.gaussian_features = None

    self.eps = eps #to avoid taking logs of zero

  def reset(self):
    self.fitted = False
    self.bernoulli_ix = []
    self.gaussian_ix = []

    self.bernoulli_likelihood_w = None
    self.gaussian_likelihood_mu = None
    self.gaussian_likelihood_s = None
    self.class_prior = None

    self.bernoulli_features = None
    self.gaussian_features = None

  def classify_features(self, X):

    self.binomial_ix = []
    self.gaussian_ix = []
    #Empty them

    for col in range(X.shape[1]):

      feature = X[:, col]
      if self.binary_checker(feature):
        self.bernoulli_ix.append(col)
      else:
        self.gaussian_ix.append(col)

    if self.bernoulli_ix:
      self.bernoulli_features = X[:, self.bernoulli_ix]

    if self.gaussian_ix:
      self.gaussian_features = X[:, self.gaussian_ix]

  def binary_checker(self, vec):
    return ((vec == 0) | (vec == 1)).all()

  def fit(self, X, y):
    assert self.binary_checker(y), "NaiveBayes is for binary classifcation problems only.\n \
    Ensure that labels are binary."
    self.reset()
    self.fitted=True

    self.classify_features(X)

    if self.verbose:
      print(self.bernoulli_ix, "bernoulli")
      print(self.gaussian_ix, "gaussian")

    #Establishing likelihood parameters
    if self.bernoulli_ix:
      self.find_bernoulli_likelihood(self.bernoulli_features, y)
    if self.gaussian_ix:
      self.find_gaussian_likelihood(self.gaussian_features, y)
    self.find_prior(y)


  def find_bernoulli_likelihood(self, X, y):
    self.bernoulli_likelihood_w = np.empty((2, X.shape[1])) #Dimensions x Classes

    self.bernoulli_likelihood_w[0, :] = np.sum(np.logical_and(X==1, y==0), axis=0) / np.sum(y==0)
    self.bernoulli_likelihood_w[1, :] = np.sum(np.logical_and(X==1, y==1), axis=0) / np.sum(y==1)

  def find_gaussian_likelihood(self, X, y):
    self.gaussian_likelihood_mu = np.empty((2, X.shape[1]))
    self.gaussian_likelihood_s = np.empty((2, X.shape[1]))

    neg_ex = np.nonzero(y==0)[0]
    pos_ex = np.nonzero(y==1)[0]

    self.gaussian_likelihood_mu[0, :] = np.mean(X[neg_ex, :], axis=0)
    self.gaussian_likelihood_mu[1, :] = np.mean(X[pos_ex, :], axis=0)

    self.gaussian_likelihood_s[0, :] = np.std(X[neg_ex, :], axis=0)
    self.gaussian_likelihood_s[1, :] = np.std(X[pos_ex, :], axis=0)

  def find_prior(self, y):
    self.class_prior = np.empty((2, 1))
    self.class_prior[0, 0] = 1 - np.mean(y, axis=0) #prior for class 0
    self.class_prior[1, 0] = np.mean(y, axis=0) #prior for class 1

  def bernoulli_log_likelihood(self, X):
    return (np.dot(np.log(self.bernoulli_likelihood_w + self.eps), X.T) + \
        np.dot(np.log(1-self.bernoulli_likelihood_w + self.eps), (1 - X).T))
  
  def gaussian_log_likelihood(self, X):
    return (- np.sum( np.log(self.gaussian_likelihood_s[:,None,:] + self.eps) +.5*(((X[None,:,:] \
        - self.gaussian_likelihood_mu[:,None,:])/self.gaussian_likelihood_s[:,None,:])**2), \
         axis=2))

  def predict(self, X):
    likelihood = np.zeros((2, X.shape[0]))
    if self.bernoulli_ix:
      likelihood = likelihood +  self.bernoulli_log_likelihood(X[:, self.bernoulli_ix])
    if self.gaussian_ix:
      likelihood = likelihood + self.gaussian_log_likelihood(X[:, self.gaussian_ix])
    
    joint = likelihood + np.log(self.class_prior + self.eps)
    return np.argmax(joint, axis=0).reshape(-1, 1)

"""## Evaluation Helper Methods"""

def evaluate_recall(y_true, y_pred):
  truePos = np.sum(np.logical_and(y_true == 1, y_pred == 1))
  falseNeg = np.sum(np.logical_and(y_true == 1, y_pred == 0))
  return np.divide( truePos, truePos + falseNeg )

def evaluate_prec(y_true, y_pred):
  truePos = np.sum(np.logical_and(y_true == 1, y_pred == 1))
  falsePos = np.sum(np.logical_and(y_true == 0, y_pred == 1))
  return np.divide( truePos, truePos + falsePos)

def evaluate_acc(y_true, y_pred):
  return np.mean(y_pred == y_true)

def evaluate(y_true, y_pred):
  acc = evaluate_acc(y_true, y_pred)
  prec = evaluate_prec(y_true, y_pred)
  recall = evaluate_recall(y_true, y_pred)
  f1 = 2 * (recall * prec) / (recall + prec)
  return {'accuracy': acc, 'precision': prec, 'recall': recall, 'f1': f1}

def evaluate_model(model, X, y):
  y_pred = model.predict(X)
  return evaluate(y, y_pred)

def k_fold_cv(k_folds, model, X, y, verbose=False):
  shuffled_X, shuffled_y = random_shuffle(X, y) #randomize ordering of data
  
  train_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}
  val_metrics = {'accuracy': [], 'precision': [], 'recall': [], 'f1': []}

  

  X_splits = np.array_split(shuffled_X, k_folds)
  y_splits = np.array_split(shuffled_y, k_folds)


  for k in range(k_folds):
  
    X_val = X_splits[k]
    y_val = y_splits[k]

    X_train = np.vstack([X_splits[i] for i, _ in enumerate(X_splits) if i != k])
    y_train = np.vstack([y_splits[i] for i, _ in enumerate(y_splits) if i != k])

    model.fit(X_train, y_train)
    y_train_pred = model.predict(X_train)
    y_val_pred = model.predict(X_val)

    train_eval = evaluate(y_train, y_train_pred)
    for key, metric in zip(train_metrics.keys(), train_eval.values()):
      train_metrics[key].append(metric)
    
    val_eval = evaluate(y_val, y_val_pred)
    for key, metric in zip(val_metrics.keys(), val_eval.values()):
      val_metrics[key].append(metric)

    if verbose:
      print(f"Fold: {k+1} | Training Accuracy: {train_metrics['accuracy'][k]} | \
        Validation Accuracy: {val_metrics['accuracy'][k]}")
    
  train_metrics['mean_accuracy'] = np.mean(train_metrics['accuracy'])
  train_metrics['mean_accuracy_err'] = np.std(train_metrics['accuracy'])
  train_metrics['mean_precision'] = np.mean(train_metrics['precision'])  
  train_metrics['mean_precision_err'] = np.std(train_metrics['precision'])
  train_metrics['mean_recall'] = np.mean(train_metrics['recall'])
  train_metrics['mean_recall_err'] = np.std(train_metrics['precision'])
  train_metrics['mean_f1'] = np.mean(train_metrics['f1'])
  train_metrics['mean_f1_err'] = np.std(train_metrics['precision'])

  val_metrics['mean_accuracy'] = np.mean(val_metrics['accuracy'])
  val_metrics['mean_accuracy_err'] = np.std(val_metrics['accuracy'])
  val_metrics['mean_precision'] = np.mean(val_metrics['precision'])
  val_metrics['mean_precision_err'] = np.std(val_metrics['accuracy'])
  val_metrics['mean_recall'] = np.mean(val_metrics['recall'])
  val_metrics['mean_recall_err'] = np.std(val_metrics['accuracy'])
  val_metrics['mean_f1'] = np.mean(val_metrics['f1'])
  val_metrics['mean_f1_err'] = np.std(val_metrics['accuracy'])


  if verbose: 
    print(f"Mean Training Accuracy: {train_metrics['mean_accuracy']} | \
    Mean Validation Accuracy: {val_metrics['mean_accuracy']}")

    print(f"Mean Training Precision: {train_metrics['mean_precision']} | \
    Mean Validation Precision: {val_metrics['mean_precision']}")

    print(f"Mean Training Recall: {train_metrics['mean_recall']} | \
    Mean Validation Recall: {val_metrics['mean_recall']}")

    print(f"Mean Training F1 Score: {train_metrics['mean_f1']} | \
    Mean Validation F1 Score: {val_metrics['mean_f1']}")

  model.fit(X, y) #retrain on all data

  return train_metrics, val_metrics


def random_shuffle(X, y):
  assert X.shape[0] == y.shape[0]
  p = np.random.permutation(X.shape[0])
  return X[p], y[p]

"""#Part 3

## Experiment Zero: Feature Comparison
"""

import pickle
def save_data(data, filename):
  with open(filename, 'wb') as fp:
      pickle.dump(data, fp, protocol=pickle.HIGHEST_PROTOCOL)
  files.download(filename)

"""While exploring the ionosphere and adult datasets, analysis provided insight for potential methods of structurally altering the datasets.

In the ionosphere dataset, features like the standard deviation and autocorrelation of the real/imaginary time series showed to be discriminative. We want to explore whether adding these new features will improve model performance.

In the adult dataset, the native country feature generates 41 new features. Conversely, encoding countries by their geographical region instead will only generate 12. This dimensionality reduction could help mitigate overfitting, or it might lead to loss of useful information.

As such, briefly, before further experimenation, we decide on the final forms of our datasets via  cross-validation comparison with Logistic Regression and Naive Bayes.
"""

## IONOSPHERE ##
X_ion = ionosphere
X_ion_new_features = ionosphere_new_features #comparison in experiment 0
y_ion = ionosphere_labels

## ADULT ##
X_adult = adult
X_adult_regions = adult_regions #comparison in experiment 0
y_adult = adult_labels

ionosphere_new_features_df

from itertools import chain, combinations

def powerset(iterable):
    s = list(iterable)
    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))

def new_features_evaluation(X, new_features, y, model):
  num_features = new_features.shape[1]
  feature_subsets = powerset(list(range(num_features)))
  subsets = []
  train_results = []
  val_results = []
  for subset in feature_subsets:
    subsets.append(subset)   
    X_new = np.hstack([X, new_features[:, subset]])

    train_metrics, val_metrics = k_fold_cv(5, model, X_new, y, verbose=False)
    train_results.append(train_metrics['accuracy'])
    val_results.append(val_metrics['accuracy'])

  return subsets, np.array(train_results), np.array(val_results)

def accuracy_by_subset(subsets, train_results, val_results, new_feature_labels):
  
  train_acc = np.mean(train_results, axis=1).reshape(-1, 1)
  val_acc = np.mean(val_results, axis=1).reshape(-1, 1)

  train_err = np.std(train_results, axis=1).reshape(-1, 1)
  val_err = np.std(val_results, axis=1).reshape(-1, 1)

  x=np.arange(train_results.shape[0]).reshape(-1, 1)
  y=np.linspace(0, 1, 21)
  width=0.35

  tick_label = []
  for subset in subsets:
    new_label = [new_feature_labels[i] for i in subset] 
    tick_label.append(' + '.join(new_label))

  fig, ax = plt.subplots()
  train_bars = ax.bar(x - width/2, train_acc, width, linewidth=0.2, label='Train Accuracy', yerr=train_err)
  val_bars = ax.bar(x + width/2, val_acc, width, linewidth=0.2, label='Validation Accuracy', yerr=val_err)

  ax.set_ylabel('% Accuracy')
  ax.set_xlabel("Included Features")
  ax.set_title('Ionosphere Accuracy with New Features')
  ax.set_xticks(x)
  ax.set_yticks(y)
  ax.set_xticklabels(tick_label, rotation=45)
  ax.legend()


  return train_acc, val_acc, train_err, val_err, fig

def exp_zero_ionosphere(X, new_features, new_feature_labels, y):
  log_rg = LogisticRegression(learning_rate=0.01, grd_iter=100000, verbose=False)
  nb = NaiveBayes()
  results = {'model': [],'figures': [], 'train_acc': [], 'val_acc': [], 
             'train_err': [], 'val_err': [], 'subsets': []}


  for model in [nb, log_rg]:
    subsets, train, val = new_features_evaluation(X, new_features, y, model)
    train_acc, val_acc, train_err, val_err, fig = accuracy_by_subset(subsets, train, val, new_feature_labels)
    results['model'].append(model)
    results['figures'].append(fig)
    results['train_acc'].append(train_acc)
    results['val_acc'].append(val_acc)
    results['train_err'].append(train_err)
    results['val_err'].append(val_err)
    results['subsets'].append(subsets)


  return results

X_ion_new_feature_labels = ['ar', 'ai', 'sr', 'si']
results_ionosphere_exp0 = exp_zero_ionosphere(X_ion, X_ion_new_features, X_ion_new_feature_labels, y_ion)

fig_1 = results_ionosphere_exp0['figures'][0]
fig_1.set_size_inches(8, 5)
fig_1.axes[0].set_title("Naive Bayes: Accuracy on Ionosphere with New Features")
fig_1

fig_2 = results_ionosphere_exp0['figures'][1]
fig_2.set_size_inches(8, 5)
fig_2.axes[0].set_title("Logistic Regression: Accuracy on Ionosphere with New Features")
fig_2

best_log_reg_ix = np.argmax(results_ionosphere_exp0['val_acc'][1])
best_log_reg_val = results_ionosphere_exp0['val_acc'][1][best_log_reg_ix]
best_log_reg_val_err = results_ionosphere_exp0['val_err'][1][best_log_reg_ix]
best_log_reg_train = results_ionosphere_exp0['train_acc'][1][best_log_reg_ix]
best_log_reg_train_err = results_ionosphere_exp0['train_err'][1][best_log_reg_ix]
best_log_reg_subset = results_ionosphere_exp0['subsets'][1][best_log_reg_ix]

print(f"The highest cross validation accuracy is: {best_log_reg_val} ", u"\u00B1", f" {best_log_reg_val_err}")
print(f"The corresponding train accuracy is: {best_log_reg_train} ", u"\u00B1", f" {best_log_reg_train_err}")
print(f"The corresponding subset of features is {[X_ion_new_feature_labels[i] for i in best_log_reg_subset]}")
#The best combination includes the imaginary standard deviation, and the real autocorrelation

print("The cross validation accuracy with no additional features is "
    f"{results_ionosphere_exp0['val_acc'][1][0]} ", u"\u00B1"
    f" {results_ionosphere_exp0['val_err'][1][0]}")

print(f"The training accuracy with no additional features is "
    f"{results_ionosphere_exp0['train_acc'][1][0]} ", u"\u00B1", 
    f" {results_ionosphere_exp0['train_err'][1][0]}")


print("The cross validation accuracy with ALL additional features is "
    f"{results_ionosphere_exp0['val_acc'][1][-1]} ", u"\u00B1"
    f" {results_ionosphere_exp0['val_err'][1][-1]}")

print(f"The training accuracy with ALL additional features is "
    f"{results_ionosphere_exp0['train_acc'][1][-1]} ", u"\u00B1", 
    f" {results_ionosphere_exp0['train_err'][1][-1]}")

best_nb_ix = np.argmax(results_ionosphere_exp0['val_acc'][0])
best_nb_val = results_ionosphere_exp0['val_acc'][0][best_nb_ix]
best_nb_val_err = results_ionosphere_exp0['val_err'][0][best_nb_ix]
best_nb_train = results_ionosphere_exp0['train_acc'][0][best_nb_ix]
best_nb_train_err = results_ionosphere_exp0['train_err'][0][best_nb_ix]
best_nb_subset = results_ionosphere_exp0['subsets'][0][best_nb_ix]

print(f"The highest cross validation accuracy is: {best_nb_val} ", u"\u00B1", f" {best_nb_val_err}")
print(f"The corresponding train accuracy is: {best_nb_train} ", u"\u00B1", f" {best_nb_train_err}")
print(f"The corresponding subset of features is {[X_ion_new_feature_labels[i] for i in best_nb_subset]}")
#The best combination includes the imaginary standard deviation, and the real autocorrelation

print("The cross validation accuracy with no additional features is "
    f"{results_ionosphere_exp0['val_acc'][0][0]} ", u"\u00B1"
    f" {results_ionosphere_exp0['val_err'][0][0]}")

print(f"The training accuracy with no additional features is "
    f"{results_ionosphere_exp0['train_acc'][0][0]} ", u"\u00B1", 
    f" {results_ionosphere_exp0['train_err'][0][0]}")

"""Since the impact on logistic regression was much more significant than in naive bayes, we use the insights from that component of the experiment. The ionosphere dataset will include features for imaginary autocorrelation, and real/imaginary standard deviation."""

def exp_zero_adult(X_countries, X_regions, y_adult):
  log_rg = LogisticRegression(learning_rate=0.01, grd_iter=100000, verbose=False)
  nb = NaiveBayes()
  results = {'model': [], 'countries_train_acc': [], 'countries_val_acc': [], 
             'countries_train_err': [], 'countries_val_err': [], 
             'regions_train_acc': [], 'regions_val_acc': [], 
             'regions_train_err': [], 'regions_val_err': [], 'figure': []}
    

  for model in [nb, log_rg]:

    countries_train_metrics, countries_val_metrics = k_fold_cv(5, model, X_countries, y_adult, verbose=False)
    regions_train_metrics, regions_val_metrics = k_fold_cv(5, model, X_regions, y_adult, verbose=False)
  
    x = np.arange(2).reshape(-1, 1)
    y = np.linspace(0, 1, 21)
    width = 0.35
    tick_label = ["Countries", "Regions"]

    train_acc = np.array([countries_train_metrics['mean_accuracy'], regions_train_metrics['mean_accuracy']]).reshape(-1, 1)
    train_err = np.array([countries_train_metrics['mean_accuracy_err'], regions_train_metrics['mean_accuracy_err']]).reshape(-1)

    val_acc = np.array([countries_val_metrics['mean_accuracy'], regions_val_metrics['mean_accuracy']]).reshape(-1, 1)
    val_err = np.array([countries_val_metrics['mean_accuracy_err'], regions_val_metrics['mean_accuracy_err']]).reshape(-1)
    
    
    fig, ax = plt.subplots()
    train_bars = ax.bar(x - width/2, train_acc, width, linewidth=0.2, label="Train Accuracy", yerr=train_err)
    val_bars = ax.bar(x + width/2, val_acc, width, linewidth=0.2, label="Validation Accuracy", yerr=val_err)


    ax.set_ylabel('% Accuracy')
    ax.set_xlabel("Adult Dataset")
    ax.set_title('Adult Accuracy with Different Country Encodings')
    ax.set_xticks(x)
    ax.set_yticks(y)
    ax.set_xticklabels(tick_label, rotation=45)
    ax.legend()

    results['model'].append(model)
    results['countries_train_acc'].append(countries_train_metrics['mean_accuracy'])
    results['countries_val_acc'].append(countries_val_metrics['mean_accuracy'])
    results['countries_train_err'].append(countries_train_metrics['mean_accuracy_err'])
    results['countries_val_err'].append(countries_val_metrics['mean_accuracy_err'])
    results['regions_train_acc'].append(regions_train_metrics['mean_accuracy'])
    results['regions_val_acc'].append(regions_val_metrics['mean_accuracy'])
    results['regions_train_err'].append(regions_train_metrics['mean_accuracy_err'])
    results['regions_val_err'].append(regions_val_metrics['mean_accuracy_err'])
    results['figure'].append(fig)


    

  return results

results_adult_exp0 = exp_zero_adult(X_adult, X_adult_regions, y_adult)

results_adult_exp0

fig_1 = results_adult_exp0['figure'][0]
fig_1.set_size_inches(10, 5)
fig_1.axes[0].set_title("Naive Bayes: Accuracy on Adult Dataset")
fig_1

fig_2 = results_adult_exp0['figure'][1]
fig_2.set_size_inches(10, 5)
fig_2.axes[0].set_title("Logistic Regression: Accuracy on Adult Dataset")
fig_2

"""Using the region encodings makes a very slight negative difference for each model, but performance is generally consistent. This shows that we're not suffering from issues of dimensionality; no obvious overfitting.

These results are close to those in the literature.

## Fetching Processed Datasets
"""

#Fetch processed datasets from part 1

## IONOSPHERE ##
X_ion = np.hstack([ionosphere, ionosphere_new_features[:, [0, 3]]]) 
#real autocorrelation, imaginary standard deviation
X_ion.shape
y_ion = ionosphere_labels

## ADULT ##
X_adult = adult #country encodings
y_adult = adult_labels

## BREAST CANCER ##
X_breastcancer = breastcancer
y_breastcancer = breastcancer_labels

## BANKNOTE AUTHENTIFICATION ##
X_banknote = banknote
y_banknote = banknote_labels

"""## Experiment A: Regularization

This experiment will explore regularization techniques. Specifically, it will compare Lasso, Ridge, Elastic Net, and early stopping. We covered Lasso and Ridge regression in class, and saw that Lasso regression is advantageous for its ability to do automated feature selection. However, Lasso regression can behave erratically, particularly in sitations with more features than examples. In thise case, Lasso is not strictly convex and may not have a unique solution. Ridge regression is stable, and won't zero out any features which could be useful in some situations where all variables contribute a small/medium effect.

Elastic Net does a simple combination of the two effects, governed by a mixing parameter. We will optimize over this mixing parameter for each dataset.

Finally, we will also compare with the performance from doing simple early stopping.


Deliverables:
* Heatmaps of grid search over elastic net parameters with validation and training accuracy
* Graph of training and validation cost with and without early stopping as a function of gradient descent iterations
* Train/validation performance numbers with and without early stopping
* Bias/variance trade-off chart? 2D, size of dots gives size of discrepany between train and val?
"""

def grid_search(lambda_penalties, ratios, X, y):
  train_grid = np.empty((len(lambda_penalties), len(ratios), 2))
  val_grid = train_grid.copy()
  for ix_outer, lambda_penalty in enumerate(lambda_penalties):
    for ix_inner, ratio in enumerate(ratios):
      print("Penalty: ", lambda_penalty, " | Ratio: ", ratio)
      log_rg = LogisticRegression(learning_rate = 0.01, grd_iter = 10000, verbose=False, 
                                  lambda_penalty=lambda_penalty, elastic_ratio = ratio)
      train_metrics, val_metrics = k_fold_cv(5, log_rg, X, y, verbose=False)

      train_grid[ix_outer, ix_inner, 0] = train_metrics['mean_accuracy']
      train_grid[ix_outer, ix_inner, 1] = train_metrics['mean_accuracy_err']

      val_grid[ix_outer, ix_inner, 0] = val_metrics['mean_accuracy']
      val_grid[ix_outer, ix_inner, 1] = val_metrics['mean_accuracy_err']

  return train_grid, val_grid


def elastic_net_expA(lambda_penalties, ratios, X_datasets, y_datasets, dataset_labels):
  
  results = {'heatmaps': [], 'train_acc': [], 
             'val_acc': [], 'train_err': [], 'val_err': [], 
             'best_val_acc': [], 'best_val_err': [],
             'best_train_acc': [], 'best_train_err': [],
             'ideal_penalty': [], 'ideal_ratio': [],
             'lambda_penalties': lambda_penalties, 'ratios': ratios}

  
  for ix, (X, y) in enumerate(zip(X_datasets, y_datasets)):
    print(f"DATASET: {dataset_labels[ix]}")

    train_grid, val_grid = grid_search(lambda_penalties, ratios, X, y)
    fig, axs = plt.subplots(1, 2, sharey=True, figsize=(10, 4))

    yticklabels = lambda_penalties
    xticklabels = ratios
    cmap="Blues"

    sns.heatmap(train_grid[:, :, 0], vmin=0.0, vmax=1.0, annot=False, 
                cmap=cmap, ax = axs[0], xticklabels=xticklabels,
                yticklabels=yticklabels, cbar_kws={'label': '% Accuracy'})
    
    sns.heatmap(val_grid[:, :, 0], vmin=0.0, vmax=1.0, annot=False, 
                cmap=cmap, ax = axs[1], xticklabels=xticklabels,
                yticklabels=yticklabels, cbar_kws={'label': '% Accuracy'})

    fig.suptitle(f"Regularization Grid: {dataset_labels[ix]}", fontsize=16, y=0.98)
    axs[0].set_xlabel("Mixing Ratio")
    axs[1].set_xlabel("Mixing Ratio")
    fig.text(0.08, 0.5, 'Lambda Penalty', ha='center', va='center', rotation='vertical')

    unrolled_ix = np.argmax(val_grid[:, :, 0])
    ideal_penalty, ideal_ratio = np.unravel_index(unrolled_ix, val_grid[:, :, 0].shape)

    results['best_val_acc'].append(val_grid[ideal_penalty, ideal_ratio, 0])
    results['best_val_err'].append(val_grid[ideal_penalty, ideal_ratio, 1])
    results['best_train_acc'].append(train_grid[ideal_penalty, ideal_ratio, 0])
    results['best_train_err'].append(train_grid[ideal_penalty, ideal_ratio, 1])

    results['ideal_penalty'].append(ideal_penalty)
    results['ideal_ratio'].append(ideal_ratio)

    
    results['heatmaps'].append(fig)
    results['train_acc'].append(train_grid[:, :, 0])
    results['train_err'].append(train_grid[:, :, 1])
    results['val_acc'].append(val_grid[:, :, 0])
    results['val_err'].append(val_grid[:, :, 1])

  return results

lambda_penalties = [0, 0.001, 0.01, 0.1, 1, 10]
mixing_ratios = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]
result_elastic_net_expA = elastic_net_expA(lambda_penalties, mixing_ratios,
                                           [X_ion, X_adult, X_breastcancer, X_banknote],
                                           [y_ion, y_adult, y_breastcancer, y_banknote],
                                           ["Ionosphere", "Adult", "Breast Cancer", "Banknote"])

result_elastic_net_expA

def cv_fit(X, y, k_folds=5, stopping_threshold=float('inf')):
    
    train_acc_by_fold = []
    val_acc_by_fold = []
    train_costs_by_fold = []
    val_costs_by_fold = []
    optimal_iter_by_fold = []

    X = np.c_[np.ones((X.shape[0], 1)), X] #add bias


    #The array will store NumPy arrays in each element
    #Each NumPy array corresponds to the train or val accuracy as a function of iterations
    #Each NumPy array thus doesn't necessarily have the same dimensions
    #Different folds might take longer to reach tolerance

    X, y = random_shuffle(X, y)

    X_splits = np.array_split(X, k_folds)   #Generate splits
    y_splits = np.array_split(y, k_folds)


    for k in range(k_folds):
        
        log_rg = LogisticRegression(learning_rate = 0.01, grd_iter = 1000000, lambda_penalty = 0, verbose=False)

        X_val = X_splits[k]
        y_val = y_splits[k]

        X_train = np.vstack([X_splits[i] for i, _ in enumerate(X_splits) if i != k])
        y_train = np.vstack([y_splits[i] for i, _ in enumerate(y_splits) if i != k])

        train_cost_history, train_accuracies, val_cost_history, val_accuracies, optimal_iteration = early_stopping(X_train, y_train, X_val, y_val, log_rg, stopping_threshold)
        #Two lists of the same length, each containing the accuracies on train/validation as a function of iterations

        train_acc_by_fold.append(train_accuracies)
        val_acc_by_fold.append(val_accuracies)
        train_costs_by_fold.append(train_cost_history)
        val_costs_by_fold.append(val_cost_history)
        optimal_iter_by_fold.append(optimal_iteration)
        #Store the lists as array entries

    return train_acc_by_fold, val_acc_by_fold, train_costs_by_fold, val_costs_by_fold, optimal_iter_by_fold




def early_stopping(X_train, y_train, X_val, y_val, model, stopping_threshold):

    train_accuracies = []
    val_accuracies = []

    model.w = np.zeros((X_train.shape[1], 1))
    model.fitted = True #Initializing a model
    
    J_val_prev = float('inf')
    val_cost_increasing = 0
    
    val_cost_history = []
    train_cost_history = []

    best_val_acc = []
    best_val_cost = []
    best_train_acc = []
    best_train_cost = []
    
    optimal_iterations = 0

    for i in range(model.grd_iter):
        
        if val_cost_increasing >= stopping_threshold:
            optimal_iteration = np.argmin(val_cost_history) #roll back to optimal cost
            break
        
        J_train = model.cost(X_train, y_train)
        train_cost_history.append(J_train)
        
        J_val = model.cost(X_val, y_val)
        val_cost_history.append(J_val)
                
        if J_val <= J_val_prev:
            val_cost_increasing = 0
        else:
            val_cost_increasing += 1

        J_val_prev = J_val

        train_accuracies.append(evaluate_acc(y_train, model.predict(X_train, add_bias=False))) #Evaluate accuracy at each iteration
        val_accuracies.append(evaluate_acc(y_val, model.predict(X_val, add_bias=False)))

        grad = model.gradients(X_train, y_train)
        model.w = model.w - model.learning_rate*grad #Gradient descent with specified learning rate
        
        optimal_iteration = i #default optimal iteration is the last one

    return np.array(train_cost_history), np.array(train_accuracies), np.array(val_cost_history), np.array(val_accuracies), optimal_iteration

def early_stopping_expA(X_datasets, y_datasets, dataset_labels):
    
    results = {'dataset': [], 'train_acc_no_es': [], 'val_acc_no_es': [], 
               'val_acc_no_es_mean': [], 'val_acc_no_es_err': [],
               'train_acc_no_es_mean': [], 'train_acc_no_es_err': [],
               'train_cost_no_es': [], 'val_cost_no_es': [],
               'train_cost_no_es_mean': [], 'train_cost_no_es_err': [],
               'val_cost_no_es_mean': [], 'val_cost_no_es_err': [],
               'no_es_optimal_iteration': [],
               'train_acc_es': [], 'val_acc_es': [], 
               'val_acc_es_mean': [], 'val_acc_es_err': [],
               'train_acc_es_mean': [], 'train_acc_es_err': [],
               'train_cost_es': [], 'val_cost_es': [],
               'train_cost_es_mean': [], 'train_cost_es_err': [],
               'val_cost_es_mean': [], 'val_cost_es_err': [],
               'es_optimal_iteration': [], 'cost_figures': [],
               'accuracy_figures': []}
    
    for ix, (X, y) in enumerate(zip(X_datasets, y_datasets)):
        print("DATASET: ", dataset_labels[ix])
        results['dataset'].append(dataset_labels[ix])

        
        train_acc_by_fold, val_acc_by_fold, train_costs_by_fold, val_costs_by_fold, \
          optimal_iter_by_fold = cv_fit(X, y, 5, float('inf'))
        
        results['train_acc_no_es'].append(train_acc_by_fold)
        results['val_acc_no_es'].append(val_acc_by_fold)
        results['train_acc_no_es_mean'].append(np.mean([train_acc_by_fold[i][j] for i, j in enumerate(optimal_iter_by_fold)]))
        results['train_acc_no_es_err'].append(np.std([train_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['val_acc_no_es_mean'].append(np.mean([val_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['val_acc_no_es_err'].append(np.std([val_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        
        results['train_cost_no_es'].append(train_costs_by_fold)
        results['val_cost_no_es'].append(val_costs_by_fold)
        results['train_cost_no_es_mean'].append(np.mean([train_costs_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['train_cost_no_es_err'].append(np.std([train_costs_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['val_cost_no_es_mean'].append(np.mean([val_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['val_cost_no_es_err'].append(np.std([val_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))

        results['no_es_optimal_iteration'].append(np.array(optimal_iter_by_fold))

        train_acc_by_fold, val_acc_by_fold, train_costs_by_fold, val_costs_by_fold, \
          optimal_iter_by_fold = cv_fit(X, y, 5, 5)
        
        results['train_acc_es'].append(train_acc_by_fold)
        results['val_acc_es'].append(val_acc_by_fold)
        results['train_acc_es_mean'].append(np.mean([train_acc_by_fold[i][j] for i, j in enumerate(optimal_iter_by_fold)]))
        results['train_acc_es_err'].append(np.std([train_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['val_acc_es_mean'].append(np.mean([val_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['val_acc_es_err'].append(np.std([val_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        
        results['train_cost_es'].append(train_costs_by_fold)
        results['val_cost_es'].append(val_costs_by_fold)
        results['train_cost_es_mean'].append(np.mean([train_costs_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['train_cost_es_err'].append(np.std([train_costs_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['val_cost_es_mean'].append(np.mean([val_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))
        results['val_cost_es_err'].append(np.std([val_acc_by_fold[i][j] for i,j in enumerate(optimal_iter_by_fold)]))

        results['es_optimal_iteration'].append(optimal_iter_by_fold)
        
        fig1, ax1 = plt.subplots(1, 2)
        for fold in range(5):
            l0 = ax1[0].plot(results['train_cost_no_es'][ix][fold], 'b-', linewidth=0.2, label="Training")
            l1 = ax1[0].plot(results['val_cost_no_es'][ix][fold], 'g-', linewidth=0.2, label="Validation")
            l2 = ax1[1].plot(results['train_cost_es'][ix][fold], 'b-', linewidth=0.2, label="Training")
            l3 = ax1[1].plot(results['val_cost_es'][ix][fold], 'g-', linewidth=0.2, label="Validation")
        

        ax1[0].legend((l0[0], l1[0]), ("Training", "Validation"))
        ax1[1].legend((l2[0], l3[0]), ("Training", "Validation"))

        fig2, ax2 = plt.subplots()
        width=0.35

        xticks = np.arange(2).reshape(-1, 1)
        yticks = np.linspace(0, 1, 21)
        width = 0.35
        tick_label = ["No Early Stopping", "Early Stopping"]

        train_acc = np.array([results['train_acc_no_es_mean'][ix], results['train_acc_es_mean'][ix]]).reshape(-1, 1)
        train_err = np.array([results['train_acc_no_es_err'][ix], results['train_acc_es_err'][ix]]).reshape(-1)

        val_acc = np.array([results['val_acc_no_es_mean'][ix], results['val_acc_es_mean'][ix]]).reshape(-1, 1)
        val_err = np.array([results['val_acc_no_es_err'][ix], results['val_acc_es_err'][ix]]).reshape(-1)

        train_bars = ax2.bar(xticks - width/2, train_acc, width, linewidth=0.2, label="Train Accuracy", yerr=train_err)
        val_bars = ax2.bar(xticks + width/2, val_acc, width, linewidth=0.2, label="Validation Accuracy", yerr=val_err)
        
        tick_labels = ["No Early Stopping", "Early Stopping"]

        ax2.set_ylabel('% Accuracy')
        ax2.set_xlabel("Regularization")
        ax2.set_title(f"Early Stopping Accuracy: {dataset_labels[ix]}")
        ax2.set_xticks(xticks)
        ax2.set_yticks(yticks)
        ax2.set_xticklabels(tick_labels, rotation=45)
        ax2.legend()

        results['cost_figures'].append(fig1)
        results['accuracy_figures'].append(fig2)

    return results

results_early_stopping_expA = early_stopping_expA([X_ion, X_adult, X_breastcancer, X_banknote],
                              [y_ion, y_adult, y_breastcancer, y_banknote],
                              ['Ionosphere', 'Adult', 'Breast Cancer', 'Banknote'])

results_early_stopping_expA

results_early_stopping_expA['val_acc_es_mean']

results_early_stopping_expA['val_acc_no_es_mean']

"""## Experiment One"""

def compare_visual(compare, err, log_rg_metrics, NB_metrics, title):

  # first compare accuracy of the each training data set for the four data sets
  labels = ['Ionosphere', 'Adult', 'Breast Cancer', 'Banknote']

  log_rg_result = [log_rg_metrics[i][compare] for i in range(len(labels))]
  nb_result = [NB_metrics[i][compare] for i in range(len(labels))]

  log_rg_err = [log_rg_metrics[i][err] for i in range(len(labels))]
  nb_err = [NB_metrics[i][err] for i in range(len(labels))]


  x = np.arange(len(labels))  # the label locations
  y = np.linspace(0, 1, 21)
  width = 0.35  # the width of the bars

  fig, ax = plt.subplots()
  rects1 = ax.bar(x - width/2, log_rg_result, width, label='Logistic Regression', yerr=log_rg_err)
  rects2 = ax.bar(x + width/2, nb_result, width, label='Naive Bayes', yerr=nb_err)
  ax.set_ylabel(compare)
  ax.set_xlabel("Dataset")

  ax.set_title(title)
  ax.set_xticks(x)
  ax.set_yticks(y)
  ax.set_xticklabels(labels)
  ax.legend()

  return fig, log_rg_result, nb_result, log_rg_err, nb_err

def exp1(X_datasets, y_datasets, dataset_labels):

  results = {"figures": []}

  train_metrics_log_rg = []
  val_metrics_log_rg = []

  train_metrics_nb = []
  val_metrics_nb = []

  log_rg = LogisticRegression(learning_rate=0.01, grd_iter=1000000, tol=1e-5)
  nb = NaiveBayes()

  for X, y in zip(X_datasets, y_datasets):
      train, val = k_fold_cv(5, log_rg, X, y)
      train_metrics_log_rg.append(train)
      val_metrics_log_rg.append(val)

  for X, y in zip(X_datasets, y_datasets):
      train, val = k_fold_cv(5, nb, X, y)
      train_metrics_nb.append(train)
      val_metrics_nb.append(val)
  

  for compare, err in zip(['mean_accuracy', 'mean_precision', 'mean_recall', 'mean_f1'],
                          ['mean_accuracy_err', 'mean_precision_err', 'mean_recall_err', 'mean_f1_err']):
    fig, log_rg_result, nb_result, log_rg_err, nb_err = compare_visual(compare, err, 
                                                                       train_metrics_log_rg, train_metrics_nb,
                                                                       f"Training: {compare}")
    results['figures'].append(fig)
    results['train_'+compare+'_log_rg'] = log_rg_result
    results['train_'+compare+'_err'+'_log_rg'] = log_rg_err
    results['train_'+compare+'_nb'] = nb_result
    results['train_'+compare+'_err'+'_nb'] = nb_err

    fig, log_rg_result, nb_result, log_rg_err, nb_err = compare_visual(compare, err, 
                                                                       val_metrics_log_rg, val_metrics_nb,
                                                                       f"Validation: {compare}")
    results['figures'].append(fig)
    results['val_'+compare+'_log_rg'] = log_rg_result
    results['val_'+compare+'_err'+'_log_rg'] = log_rg_err
    results['val_'+compare+'_nb'] = nb_result
    results['val_'+compare+'_err'+'_nb'] = nb_err

  return results

results_exp1 = exp1([X_ion, X_adult, X_breastcancer, X_banknote], [y_ion, y_adult, y_breastcancer, y_banknote],
                    ["Ionosphere", "Adult", "Breastcancer", "Banknote"])

results_exp1

"""## Experiment Two"""

def learning_rate_cv(X, y, learning_rate, k_folds=5):

  train_acc_by_fold = []
  val_acc_by_fold = []

  #The array will store NumPy arrays in each element
  #Each NumPy array corresponds to the train or val accuracy as a function of iterations
  #Each NumPy array thus doesn't necessarily have the same dimensions! Different folds might take longer to reach tolerance

  X, y = random_shuffle(X, y)

  X_splits = np.array_split(X, k_folds)   #Generate splits
  y_splits = np.array_split(y, k_folds)


  for k in range(k_folds):
  
    X_val = X_splits[k]
    y_val = y_splits[k]

    X_train = np.vstack([X_splits[i] for i, _ in enumerate(X_splits) if i != k])
    y_train = np.vstack([y_splits[i] for i, _ in enumerate(y_splits) if i != k])

    train_acc_by_iter, val_acc_by_iter = test_learning_rate(X_train, y_train, X_val, y_val, learning_rate)
    #Two lists of the same length, each containing the accuracies on train/validation as a function of iterations

    train_acc_by_fold.append(np.array(train_acc_by_iter))
    val_acc_by_fold.append(np.array(val_acc_by_iter))

    #Store the arrays as list entries
  
  return train_acc_by_fold, val_acc_by_fold
    



def test_learning_rate(X_train, y_train, X_val, y_val, learning_rate):
  max_iterations = 100000 #Hard coded maximum so we don't run forever
  tol= 1e-4 #Tiny number. 
  #Although LogisticRegression has built-in tolerance stopping, we do it manually
  #This is because we have to record accuracies after each iteration, so can't use the fit method
  train_accuracies = []
  val_accuracies = []


  log_rg = LogisticRegression(learning_rate = learning_rate, grd_iter = max_iterations, 
                              tol = tol, lambda_penalty = 0)
  log_rg.w = np.zeros((X_train.shape[1], 1))
  log_rg.fitted = True #Initializing a model

  diff = float("inf")
  J_prev = float("inf")
  iterations = 0

  while diff > tol and iterations < max_iterations: 

    train_accuracies.append(evaluate_acc(y_train, log_rg.predict(X_train, add_bias=False))) #Evaluate accuracy at EACH ITERATION
    val_accuracies.append(evaluate_acc(y_val, log_rg.predict(X_val, add_bias=False)))

    grad = log_rg.gradients(X_train, y_train)
    log_rg.w = log_rg.w - learning_rate*grad #Gradient descent with specified learning rate

    J = log_rg.cost(X_train, y_train)
    diff = abs(J - J_prev) #For stopping at tolerance

    iterations += 1
  
  return train_accuracies, val_accuracies



def exp_2(X_datasets, y_datasets, dataset_labels, learning_rates):
  dataset_dict = {'figures': [], 'train_acc': [], 'val_acc': []}
  results = {'learning_rates': learning_rates}
  for ix, (X, y) in enumerate(zip(X_datasets, y_datasets)):
    label = dataset_labels[ix]
    print(f"DATASET: {label}")
    results[label] = dataset_dict
    for rate in learning_rates:
        print(f"Learning Rate: {rate}")
        train_accuracies_by_fold, val_accuracies_by_fold = learning_rate_cv(X, y, rate, 5)
        results[label]['train_acc'].append(train_accuracies_by_fold)
        results[label]['val_acc'].append(val_accuracies_by_fold)
        fig, ax = plt.subplots()
        ax.set_title(f"{label} - Learning Rate: {rate}")
        ax.set_ylabel("% Accuracy")
        ax.set_xlabel("Gradient Descent Iteration")
        ax.set_yticks(np.linspace(0, 1, 21))

        ax.plot(train_accuracies_by_fold[0], 'b-', linewidth=0.2, label="Training")
        ax.plot(val_accuracies_by_fold[0], 'g-', linewidth=0.2, label="Validation")
        ax.legend()

        for train, val in zip(train_accuracies_by_fold[1:], val_accuracies_by_fold[1:]):
          ax.plot(train, 'b-', linewidth=0.2)
          ax.plot(val, 'g-', linewidth=0.2)
        
        results[label]['figures'].append(fig)

  return results

learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]

results_exp2 = exp_2([X_ion, X_adult, X_breastcancer, X_banknote], 
                    [y_ion, y_adult, y_breastcancer, y_banknote],
                    ["Ionosphere", "Adult", "Breast Cancer", "Banknote"],
                    learning_rates)

"""## Experiment Three"""

def train_test_split(X, y, ratio=0.8):
  X, y = random_shuffle(X, y)
  split_ix = int(X.shape[0] * 0.8)
  X_train = X[0:split_ix, :]
  X_test = X[split_ix:, :]
  y_train = y[0:split_ix]
  y_test = y[split_ix:]
  return X_train, X_test, y_train, y_test

def exp_3(X_datasets, y_datasets, dataset_labels, train_sizes):
  results = {'train_sizes': train_sizes, 'dataset': dataset_labels, 
             'train_acc_log_rg': [], 'train_err_log_rg': [], 'val_acc_log_rg': [], 
             'val_err_log_rg': [], 'test_acc_log_rg': [], 'train_acc_nb': [], 
             'train_err_nb': [], 'val_acc_nb': [], 'val_err_nb': [], 
             'test_acc_nb': [],'figures': []}

  log_rg = LogisticRegression(0.01, 100000)
  nb = NaiveBayes()

  for ix, (X, y) in enumerate(zip(X_datasets, y_datasets)):
    label = dataset_labels[ix]
    X_train, X_test, y_train, y_test = train_test_split(X, y)
    train_acc_log_rg = []
    train_err_log_rg = []
    val_acc_log_rg = []
    val_err_log_rg = []
    test_acc_log_rg = []

    train_acc_nb = []
    train_err_nb = []
    val_acc_nb = []
    val_err_nb = []
    test_acc_nb = []



    for train_size in train_sizes:
      X_train, y_train = random_shuffle(X_train, y_train)
      train_ix = int(X_train.shape[0] * train_size)
      X_train_actual = X_train[:train_ix, :]
      y_train_actual = y_train[:train_ix]

      train_metrics, val_metrics = k_fold_cv(5, log_rg, X_train_actual, y_train_actual)
      train_acc_log_rg.append(train_metrics['mean_accuracy'])
      train_err_log_rg.append(train_metrics['mean_accuracy_err'])
      val_acc_log_rg.append(val_metrics['mean_accuracy'])
      val_err_log_rg.append(val_metrics['mean_accuracy_err'])

      test_acc_log_rg.append(evaluate_acc(y_test, log_rg.predict(X_test)))

      train_metrics, val_metrics = k_fold_cv(5, nb, X_train_actual, y_train_actual)
      train_acc_nb.append(train_metrics['mean_accuracy'])
      train_err_nb.append(train_metrics['mean_accuracy_err'])
      val_acc_nb.append(val_metrics['mean_accuracy'])
      val_err_nb.append(val_metrics['mean_accuracy_err'])

      test_acc_nb.append(evaluate_acc(y_test, nb.predict(X_test)))

    fig, axs = plt.subplots(1, 3, sharey=True, figsize=(10, 5))
    fig.suptitle(f"{label}: Train, Validation, and Test Accuracy", y=1.00, fontsize=16)
    fig.text(0.05, 0.5, '%Accuracy', ha='center', va='center', rotation='vertical')
    

    linewidth = 0.2
    log_rg_edgecolor = '#CC4F1B'
    log_rg_facecolor= '#089FFF'
    log_rg_fmt = 'b-'

    nb_edgecolor = '#1B2ACC'
    nb_facecolor='#95dbb7'
    nb_fmt = 'g-'
    alpha=0.2

    xlabel = "% of Training Set"

    y=np.linspace(0, 1, 21)

    axs[0].plot(train_sizes, train_acc_log_rg, log_rg_fmt, linewidth=linewidth, label="Logistic Regression")
    axs[0].fill_between(train_sizes, np.array(train_acc_log_rg)-np.array(train_err_log_rg), 
                        np.array(train_acc_log_rg)+np.array(train_err_log_rg), alpha=alpha, 
                        facecolor=log_rg_facecolor)

    axs[0].plot(train_sizes, train_acc_nb, nb_fmt, linewidth=linewidth, label="Naive Bayes")
    axs[0].fill_between(train_sizes, np.array(train_acc_nb)-np.array(train_err_nb), 
                        np.array(train_acc_nb)+np.array(train_err_nb), alpha=alpha, 
                        facecolor=nb_facecolor)


    axs[0].set_yticks(y)
    axs[0].set_title(f"{label} Training Set")
    axs[0].legend()


    axs[1].plot(train_sizes, val_acc_log_rg, log_rg_fmt, linewidth=linewidth, label="Logistic Regression")
    axs[1].fill_between(train_sizes, np.array(val_acc_log_rg)-np.array(val_err_log_rg), 
                        np.array(val_acc_log_rg)+np.array(val_err_log_rg), alpha=alpha, 
                        facecolor=log_rg_facecolor)

    axs[1].plot(train_sizes, val_acc_nb, nb_fmt, linewidth=linewidth, label="Naive Bayes")
    axs[1].fill_between(train_sizes, np.array(val_acc_nb)-np.array(val_err_nb), 
                        np.array(val_acc_nb)+np.array(val_err_nb), alpha=alpha, 
                        facecolor=nb_facecolor)
    
    axs[1].set_title(f"{label} Validation Set")
    axs[1].legend()


    axs[2].plot(train_sizes, test_acc_log_rg, log_rg_fmt, linewidth=linewidth, label="Logistic Regression")
    axs[2].plot(train_sizes, test_acc_nb, nb_fmt, linewidth=linewidth, label="Naive Bayes")
    axs[2].set_title(f"{label} Test Set")
    axs[2].legend()

    

    results['figures'].append(fig)



    results['train_acc_log_rg'].append(train_acc_log_rg)
    results['train_err_log_rg'].append(train_err_log_rg)
    results['val_acc_log_rg'].append(val_acc_log_rg)
    results['val_err_log_rg'].append(val_err_log_rg)

    results['test_acc_log_rg'].append(test_acc_log_rg)

    results['train_acc_nb'].append(train_acc_nb)
    results['train_err_nb'].append(train_err_nb)
    results['val_acc_nb'].append(val_acc_nb)
    results['val_acc_nb'].append(val_acc_nb)

    results['test_acc_nb'].append(test_acc_nb)

sizes = np.linspace(0.1, 1, 100)
results_exp3 = exp_3([X_ion, X_adult, X_breastcancer, X_banknote],
                      [y_ion, y_breastcancer, y_banknote],
                      ["Ionosphere", "Adult", "Breast Cancer", "Banknote"],
                      sizes)



results_exp3

